{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implements a soft policy exploration Monte Carlo estimation algorithm\n",
    "# to compute the optimal action-value function for the racetrack example.\n",
    "# \n",
    "# Mods to try:\n",
    "# 0) Implement recursive averaging update of Q\n",
    "# \n",
    "#    Q_{k+1} <- Q_k + (1/(k+1)) (r_{k+1} - Q_{k}) \n",
    "# \n",
    "# Result: \n",
    "# \n",
    "# 1) Add a fixed step size learning algorithm,\n",
    "#\n",
    "#    Q_{k+1} <- Q_k + \\alpha (r_{k+1} - Q_{k}) \n",
    "# \n",
    "#  which should be better for problems where the action value function may\n",
    "#  change over time which is the case when we are performing value iteration.\n",
    "# \n",
    "# Result: \n",
    "# \n",
    "# 2) Set Q initially very large to encourage exploration.  A value ~ +5\n",
    "# should be large enough. \n",
    "#\n",
    "# Results:\n",
    "# \n",
    "# With geometric update (involving alpha):\n",
    "# \n",
    "# With recursive update: \n",
    "# \n",
    "# the fixed step size learning parameter: \n",
    "alpha = 0.1; \n",
    "\n",
    "#N_EPISODES=10;     % a numerical approximation of +Inf\n",
    "N_EPISODES=100;\n",
    "#N_EPISODES=2e3;    # 17 seconds \n",
    "#N_EPISODES=5e5;    # 1 hour \n",
    "#N_EPISODES=1.2e7;  # <- should take 24 hours ... \n",
    "\n",
    "\n",
    "# generate the race track and initialize some sizes: \n",
    "RT = mk_rt()\n",
    "maxNPii, maxNPjj = RT.shape\n",
    "\n",
    "# the dimensions of the velocity state: \n",
    "maxNVii = 6\n",
    "maxNVjj = 6 \n",
    "\n",
    "# the dimensions of the possible actions: \n",
    "maxNAii = 3\n",
    "maxNAjj = 3\n",
    "\n",
    "# the maximal state/action dimenions: \n",
    "# \n",
    "# a state consists of [pii,pjj,vii,vjj] with \n",
    "# pii \\in maxNPii, pjj \\in maxNPjj, vii \\in 0:5, vjj \\in 0:5\n",
    "maxNStates  = np.prod([maxNPii,maxNPjj,maxNVii,maxNVjj])   #  ~ 9216 states!\n",
    "maxNActions = np.prod([maxNAii,maxNAjj]) \n",
    "\n",
    "# storage for the objects we will calculate: \n",
    "Q = np.zeros((maxNStates,maxNActions))         # the initial action-value function\n",
    "#Q = +5*ones(maxNStates,maxNActions);      % the initial action-value function taken to encourage exploration\n",
    "#Q = the_valid_spots; \n",
    "\n",
    "firstSARewSum = np.zeros(maxNStates,maxNActions) \n",
    "firstSARewCnt = np.zeros(maxNStates,maxNActions)\n",
    "\n",
    "#timePerPlay   = zeros(1,N_EPISODES); \n",
    "\n",
    "# enumerate the possible starting locations: \n",
    "posStarts = np.where(RT[-1,:] == 1)\n",
    "nPosStarts = posStarts.shape[0]\n",
    "\n",
    "# initialize our policy: \n",
    "pol_pi        = npzeros((maxNStates,maxNActions))         # the storage for our initial policy\n",
    "#pol_pi = init_unif_policy(RT, maxNStates,maxNActions,maxNPii,maxNPjj,maxNVii,maxNVjj,maxNAii,maxNAjj);\n",
    "\n",
    "\n",
    "for ei in xrange(N_EPISODES):\n",
    "  \n",
    "    # (A) generate an episode following the policy pol_pi: \n",
    "   \n",
    "    [stateseen,act_taken,rew] = gen_rt_episode(ei,pol_pi, RT,posStarts,nPosStarts,maxNStates,maxNActions,maxNPii,maxNPjj,maxNVii,maxNVjj,maxNAii,maxNAjj); \n",
    "\n",
    "  % (B) estimate the action value function \"Q\" via monte carlo methods:\n",
    "  % \n",
    "  [Q,firstSARewCnt,firstSARewSum] = mcEstQ(stateseen,act_taken,rew, firstSARewCnt,firstSARewSum,Q, maxNPii,maxNPjj,maxNVii,maxNVjj); \n",
    "  \n",
    "  % (C) update our policy:\n",
    "  % \n",
    "  [pol_pi] = rt_pol_mod(stateseen,Q, pol_pi, maxNPii,maxNPjj,maxNVii,maxNVjj,maxNAii,maxNAjj);\n",
    "  \n",
    "end % end number of episods loop \n",
    "toc\n",
    "\n",
    "%fprintf('timePerPlay = %f\\n',mean(timePerPlay)); \n",
    "\n",
    "\n",
    "% plot the learned action-value function Q^{*} \n",
    "% ... skipped for now \n",
    "\n",
    "\n",
    "% plot the learned state-value function V^{*} (greedy from Q) as a function of position ONLY:\n",
    "% \n",
    "% This means that we average out \n",
    "% -- the action variables in Q\n",
    "% -- the velocity state variables (vxx,vyy)\n",
    "%  \n",
    "% We assume that 0.0 are variables that have NOT been updated and are INACCESABLE states ... \n",
    "% \n",
    "% to just look at the POSITION part of the state value function\n",
    "% \n",
    "Q( find(Q(:)==0.0) ) = NaN;                           % <- replace zeros with NaN's\n",
    "V = nanmean( Q, 2 );                                  % <- average out the action variables\n",
    "V( find(isnan(V(:))) ) = 0.0;                         % <- replace back with zeros\n",
    "V = reshape( V, [maxNPii,maxNPjj,maxNVii,maxNVjj] );  % <- do the same for the velocities ... \n",
    "V( find(V(:)==0.0) ) = NaN; \n",
    "V = nanmean( V, 4 ); \n",
    "V = nanmean( V, 3 ); \n",
    "V( find(isnan(V(:))) ) = 0.0; \n",
    "\n",
    "figure; imagesc( V ); colorbar; \n",
    "xlabel( 'jj location' ); ylabel( 'ii location' ); \n",
    "drawnow; \n",
    "saveas( gcf, sprintf('avg_state_value_fn_%d',N_EPISODES), 'png' );\n",
    "\n",
    "return; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [stateseen, act_taken, rew] = gen_rt_episode(ei,pol_pi,RT,posStarts,nPosStarts,maxNStates,maxNActions,maxNPii,maxNPjj,maxNVii,maxNVjj,maxNAii,maxNAjj)\n",
    "% GEN_RT_EPISODE - Generates a RT episode\n",
    "% \n",
    "% Note: this version is not ness. very strict with regards to whether we jump over \n",
    "% corners of our track.  A better version would calculate the furthest we could go\n",
    "% before we intersect the edge and then use that as a starting point for the next iteration.\n",
    "% \n",
    "% In any event this version does provide an \"environment\" even if somewhat strange in which \n",
    "% our reinforcement algorithm can operate and the needed modifications to make this more \n",
    "% realistic done at any time.\n",
    "% \n",
    "% Written by:\n",
    "% -- \n",
    "% John L. Weatherwax                2007-12-07\n",
    "% \n",
    "% email: wax@alum.mit.edu\n",
    "% \n",
    "% Please send comments and especially bug reports to the\n",
    "% above email address.\n",
    "% \n",
    "%-----\n",
    "\n",
    "PLT_SPS=0; PRNT_STUFF=0; \n",
    "\n",
    "if( PLT_SPS ) \n",
    "  figure; imagesc(RT); colorbar; hold on; \n",
    "end\n",
    "\n",
    "rew=0; stateseen = []; act_taken = []; \n",
    "\n",
    "% pick an initial starting state, position and velocity (using exploring starts): \n",
    "%ii=maxNPii; tmp = randperm(nStarts); jj=posStarts(tmp(1)); clear tmp; vii = 0; vjj = 0; \n",
    "\n",
    "% the initial position: \n",
    "pii=maxNPii; pjj=posStarts(mod(ei,nPosStarts)+1); \n",
    "% the initial velocity: \n",
    "vii=mod(ei,maxNVii); vjj=mod(ei,maxNVjj); \n",
    "% correct (vii,vjj) if we happen to have selected (0,0):\n",
    "if( vii==0 && vjj==0 )\n",
    "  if( unidrnd(2)==1 )\n",
    "    vii=mod(ei,maxNVii-1)+1; \n",
    "  else\n",
    "    vjj=mod(ei,maxNVjj-1)+1;\n",
    "  end\n",
    "end\n",
    "\n",
    "if( PLT_SPS ) plot(pjj,pii,'x'); end; \n",
    "\n",
    "% accumulate/store the first state seen: \n",
    "stateseen(1,:) = [ pii,pjj,vii,vjj ];\n",
    "\n",
    "% implement a full episode following the policy specified by pol_pi:\n",
    "while( 1 ) %~didWeFinish([pii,pjj,vii,vjj],maxNPjj) ) % take a step \n",
    "  stInd       = sub2ind( [maxNPii,maxNPjj,maxNVii,maxNVjj], pii,pjj,vii+1,vjj+1 ); \n",
    "  reshape(pol_pi(stInd,:),[3,3]); \n",
    "  act_to_take = sample_discrete( pol_pi(stInd,:), 1, 1 );\n",
    "  act_taken   = [ act_taken; act_to_take ]; \n",
    "  [aIndii,aIndjj] = ind2sub( [ maxNAii,maxNAjj ], act_to_take ); \n",
    "  aii = aIndii-2; ajj = aIndjj-2; % the specific actions to take \\in {-1,0,+1} \n",
    "  % update our state according to this action and recieve a reward: \n",
    "  vii=vii+aii; \n",
    "  vjj=vjj+ajj; \n",
    "  if( vii<0 || vii>5 )\n",
    "    [pii,pjj,vii-aii,vjj-ajj,aii,ajj]\n",
    "    error( 'vii out of bounds' ); \n",
    "  end\n",
    "  if( vjj<0 || vjj>5 )\n",
    "    [pii,pjj,vii-aii,vjj-ajj,aii,ajj]\n",
    "    error( 'vjj out of bounds' ); \n",
    "  end  \n",
    "  pii=pii-vii;\n",
    "  pjj=pjj+vjj; \n",
    "  \n",
    "  if( didWeFinish([pii,pjj,vii,vjj],maxNPjj) ) break; end\n",
    "  \n",
    "  % add a random VALID component to our step: \n",
    "  rndUp=0; rndRt=0; \n",
    "  if( rand < 0.5 ) % we have a random step \n",
    "    if( rand < 0.5 ) % that is up \n",
    "      pii=pii-1; if( pii>0         ) rndUp=1; else, pii=pii+1; end \n",
    "    else             % that is right \n",
    "      pjj=pjj+1; if( pjj<maxNPjj+1 ) rndRt=1; else, pjj=pjj-1; end \n",
    "    end\n",
    "  end\n",
    "  %--\n",
    "  % Now the \"environment\" responds to this action taken from this state: \n",
    "  %--\n",
    "  if( PRNT_STUFF ) \n",
    "    fprintf('[pii,pjj]=[%d,%d]...\\n',pii,pjj); \n",
    "    fprintf('onRT(pii,pjj)=%d...\\n',onRT(pii,pjj,RT,maxNPii,maxNPjj) ); \n",
    "    fprintf('onRT(pii-1,pjj)=%d...\\n',onRT(pii-1,pjj,RT,maxNPii,maxNPjj) ); \n",
    "    fprintf('onRT(pii,pjj+1)=%d...\\n',onRT(pii,pjj+1,RT,maxNPii,maxNPjj) ); \n",
    "    fprintf('onRT(pii-1,pjj+1)=%d...\\n',onRT(pii-1,pjj+1,RT,maxNPii,maxNPjj) ); \n",
    "  end\n",
    "  if( onRT(pii,pjj,RT,maxNPii,maxNPjj) )\n",
    "    rew = rew-1; \n",
    "  else\n",
    "    rew = rew-5; \n",
    "    % adjust our position if we fall off the track:\n",
    "    % first obtain our original position (which was valid):\n",
    "    pii = pii + vii; if( rndUp ) pii=pii+1; end\n",
    "    pjj = pjj - vjj; if( rndRt ) pjj=pjj-1; end \n",
    "    if( PRNT_STUFF ) \n",
    "      fprintf('[pii,pjj]=[%d,%d]...\\n',pii,pjj); \n",
    "      fprintf('onRT(pii,pjj)=%d...\\n',onRT(pii,pjj,RT,maxNPii,maxNPjj) ); \n",
    "    end\n",
    "    % adjust our velocity back to what we originally had: \n",
    "    %vii=vii-aii; vjj=vjj-ajj; \n",
    "    \n",
    "    % find a valid next spot: \n",
    "    if( onRT(pii-1,pjj,RT,maxNPii,maxNPjj) )\n",
    "      pii=pii-1;\n",
    "    elseif( onRT(pii,pjj+1,RT,maxNPii,maxNPjj) )\n",
    "      pjj=pjj+1; \n",
    "    elseif( onRT(pii-1,pjj+1,RT,maxNPii,maxNPjj) )\n",
    "      pii=pii-1; pjj=pjj+1; \n",
    "    else\n",
    "      %error( 'can''t find a valid next spot' ); \n",
    "      % as a last ditch we will continue from our original spot (which by assumption was valid)\n",
    "      % sometimes we can fall of fthe RT by way of finishing ... \n",
    "      % do nothing ... (pii, pjj) are set to a valid spot \n",
    "    end    \n",
    "  end\n",
    "\n",
    "  if( PLT_SPS ) plot(pjj,pii,'x'); end; \n",
    "  \n",
    "  stateseen(end+1,:) = [ pii,pjj,vii,vjj ];\n",
    "end\n",
    "\n",
    "return; \n",
    "\n",
    "\n",
    "\n",
    "function onQ = onRT(pii,pjj,RT,maxNPii,maxNPjj)\n",
    "% onRT - \n",
    "%   \n",
    "if( ~( (1<=pii) && (pii<=maxNPii) ) )\n",
    "  onQ = 0; \n",
    "  return; \n",
    "end\n",
    "if( ~( (1<=pjj) && (pjj<=maxNPjj) ) )\n",
    "  onQ = 0; \n",
    "  return; \n",
    "end\n",
    "if( RT(pii,pjj)==1 )\n",
    "  onQ = 1; \n",
    "  return; \n",
    "else\n",
    "  onQ = 0; \n",
    "  return; \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function finishQ = didWeFinish(st,maxNPjj)\n",
    "% DIDWEFINISH - \n",
    "%   \n",
    "pii = st(1); pjj = st(2); \n",
    "\n",
    "if( pjj>=maxNPjj )\n",
    "  finishQ = 1; \n",
    "else\n",
    "  finishQ = 0; \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mk_rt():\n",
    "    # MK_RT - Makes the RT for the RT example\n",
    "    A = np.zeros((16,16)); \n",
    "    A[15,3:12] = 1; \n",
    "    A[14,3:12] = 1; \n",
    "    A[9:14,4:12] = 1; \n",
    "    A[7:9,7:12] = 1; \n",
    "    A[6,6:13] = 1; \n",
    "    A[5,7:13] = 1; \n",
    "    A[4,7:16] = 1; \n",
    "    A[3,7:16] = 1; \n",
    "    A[2,8:16] = 1; \n",
    "    A[1,10:16] = 1; \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_rt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
